{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BBM409 - Machine Learning Lab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I : Theory Questions\n",
    "#### MLE\n",
    " - Suppose you have N samples $x_1, x_2,...,x_N$ from a univariate normal distribution with unknown mean $\\mu$ and known variance $ \\sigma^2  $. Derive the MLE estimator for the mean $\\mu$.\n",
    "\n",
    "##### Solution:\n",
    "Derive the likelihood term : \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{equation*}\n",
    "P(x_1,..,x_N|\\mu) =  \\prod_{i=1}^N  P(x_i |\\mu)\n",
    " = \\prod_{i=1}^N \\frac{1}{\\sqrt {2\\pi\\sigma^2}}e^{-\\frac{(x_i - \\mu)^2}{2\\sigma^2}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximize the log-likelihood:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\log{(P(x_1,..,x_N|\\mu))} = \\sum_{i=1}^N \\log{(\\frac{1}{\\sqrt {2\\pi\\sigma^2}})} - \\frac{(x_i - \\mu)^2}{2\\sigma^2}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take derivatives of this with respect to $\\mu$ and find:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{d\\log{(P(x_1,..,x_N|\\mu))}}{d\\mu} = \\sum_{i=1}^N \\frac{(x_i-\\mu)}{\\sigma^2}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the left hand side equal to zero:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "0 = \\sum_{i=1}^N \\frac{(x_i-\\mu)}{\\sigma^2}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\sum_{i=1}^N \\mu = \\sum_{i=1}^N x_i\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "N\\mu = \\sum_{i=1}^N x_i\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\mu = \\frac{\\sum_{i=1}^N x_i}{N}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Consider a dataset ($x^n; c^n$); n = 1,2,...,N of binary attributes, $x^n_i \\epsilon$ 0,1; i = 1,...,D\n",
    "and associated class label $c^n$. The number of datapoints from class c = 0 is denoted\n",
    "$n_0$ and the number from class c = 1 is denoted n1. Estimate $p(x_i = 1|c)\\equiv \\theta_i^c$.\n",
    "##### Solution:\n",
    "\n",
    "The other probability, $p(x_i = 0|c)$ is given by the normalization requirement, $p(x_i = 0|c)=1 - p(x_i = 1|c) = 1-\\theta_i^c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on Naive Bayes conditional independence assumption the probability of observing a vector x can be compactly written:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(x|c) = \\prod_{i=1}^D p(x_i|c) = \\prod_{i=1}^D (\\theta_i^c)^{x_i} (1-\\theta_i^c)^{1-x_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together with the assumption that the training data is i.i.d. generating, the log likelihood of the attributes and class labels is:\n",
    "\n",
    "\\begin{equation*}\n",
    "L = \\sum_n \\log{p(x^n,c^n)} =  \\sum_{n} \\log{p(c^n)\\prod_{i}{p(x_i^n|c^n)}}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "  = \\{ \\sum_{i,n} x_i^n \\log{\\theta_i^{c^n}} + (1-x_i^n)\\log{(1-\\theta_i^{c^n})}\\} + n_0 \\log{p(c=0)} + n_1 \\log{p(c=1)}\n",
    "\\end{equation*}\n",
    "\n",
    "This can be written more explicitly:\n",
    "\n",
    "\\begin{equation*}\n",
    "L = \\sum_{i,n} \\{ \\parallel [ x_i^n = 1, c^n =0]\\log{\\theta_i^0} + \\parallel[x_i^n = 0 , c^n = 0]\\log{(1-\\theta_i^0)} + \\parallel [ x_i^n=1,c^n=1] \\log{\\theta_i^1} + \\parallel [x_i^n = 0,c^n = 1] \\log{(1-\\theta_i^1)}\\} + n_0 \\log{p(c=0)} + n_1\\log{p(c=1)}\n",
    "\\end{equation*}\n",
    "\n",
    "Find the maximum likelihood optimal $\\theta_i^c$ by differentiating and equating to zero, result:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\theta_i^c = p(x_i =1|c) = \\frac{\\sum_n \\parallel[x_i^n = 1,c^n=c]}{\\sum_n \\parallel [x_i^n =0,c^n=c]+\\parallel[x_i^n=1,c^n=c]}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Suppose Suppose that X is a discrete rrandom variable with the following probability mass function: where 0 $\\leq\\theta\\leq$ 1 is a parameter. The following 10 independent observations were taken from such a distribution: (3,0,2,1,3,2,1,0,2,1). What is the maximum likelihood estimate of $\\theta$.\n",
    " \n",
    "##### Solution:\n",
    "\n",
    "\\begin{equation*}\n",
    "L(\\theta) = P(X=3)P(X=0)P(X=2)P(X=1)P(X=3)P(X=2)P(X=1)P(X=0)P(X=2)P(X=1)\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "L(\\theta) = \\prod_{i=1}^n P(X_i|\\theta) = \\bigg(\\frac{2\\theta}{3}\\bigg)^2 \\bigg(\\frac{\\theta}{3}\\bigg)^3 \\bigg(\\frac{2(1-\\theta)}{3}\\bigg)^3 \\bigg(\\frac{1-\\theta}{3}\\bigg)^2\n",
    "\\end{equation*}\n",
    "\n",
    "###### Note: The likelihood function L($\\theta$) is not easy to maximize\n",
    "\n",
    "\\begin{equation*}\n",
    "l(\\theta) = \\log{L(\\theta)} = \\sum_{i=1}^n\\log{P(X_i|\\theta)}\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "   = 2\\bigg(\\log{\\frac{2}{3}}+\\log{\\theta}\\bigg) + 3\\bigg(\\log{\\frac{1}{3}}+\\log{\\theta}\\bigg) + 3\\bigg(\\log{\\frac{2}{3}}+\\log{(1-\\theta)}\\bigg) + 2\\bigg(\\log{\\frac{1}{3}}+\\log{(1-\\theta)}\\bigg)\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "= C + 5\\log{\\theta} + 5\\log{(1-\\theta)}\n",
    "\\end{equation*}\n",
    "\n",
    "Let the derivative of function l($\\theta$) with respect to $\\theta$ be zero:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{d l(\\theta)}{d \\theta} = \\frac{5}{\\theta}-\\frac{5}{1-\\theta} = 0\n",
    "\\end{equation*}\n",
    "\n",
    "It gives us the MLE, $\\theta$ = 0.5.\n",
    "\n",
    "#### NAIVE BAYES\n",
    "\n",
    " - Using Naive Bayes, what is the probability that a person who is 'not rich', 'married' and 'healthy' is 'content'?\n",
    "<br><br>\n",
    "$ P(content|notrich, married, healthy) = \\frac{P(notrich, married, healthy|content)}{P(notrich, married healthy)} $\n",
    "<br><br>\n",
    "$ P(notrich, married, healthy|content) = P(notrich|content) * P(married|content) * P(healthy|content) * P(content) $\n",
    "<br><br>\n",
    "$ P(notrich, married, healthy|content) = \\frac{1}{4} * \\frac{2}{4} * \\frac{3}{4} * \\frac{4}{9} = 0.0417 $\n",
    "<br><br>\n",
    "$ P(notrich, married, healty) = P(notrich, married, healty|content) + P(notrich, married, healty|notcontent) $\n",
    "<br><br>\n",
    "$ P(notrich, married, healty|notcontent) = P(notrich|notcontent) * P(married|notcontent) * P(healthy|notcontent) * P(notcontent) $\n",
    "<br><br>\n",
    "$ P(notrich, married, healty|notcontent) = \\frac{4}{5} * \\frac{1}{5} * \\frac{1}{5} * \\frac{5}{9} = 0.0178 $\n",
    "<br><br>\n",
    "$ P(content|notrich, married, healthy) = \\frac{0.0417}{0.0178} = 0.7 $\n",
    "<br><br>\n",
    "Which means the probability is **%70** percent\n",
    "<br><br><br><br>\n",
    " - What is the probability that a person who is 'not rich' and 'married' is 'content'? (That is, we do not know whether ot not they are 'healthy'.)\n",
    "<br><br>\n",
    "$ P(notrich, married|content) = \\frac{1}{4} * \\frac{2}{4} * \\frac{4}{9} = 0.0556 $\n",
    "<br><br>\n",
    "$ P(notrich, married|notcontent) = \\frac{4}{5} * \\frac{1}{5} * \\frac{5}{9} = 0.0889 $\n",
    "<br><br>\n",
    "$ P(content|notrich, married) = \\frac{0.0556}{0.0889} =  0.385 $\n",
    "<br><br>\n",
    "Which means the probability is **%38.5** percent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: Detection of Fake News\n",
    "#### 1. Understanding Data\n",
    "The number of times in the given fake and real datasets, it is decided whether these news are real or fake. Because when you look at the number of words, there are too many differences between them. And this may be enough for us to decide. Below samples are available.\n",
    "\n",
    "For Real dataset:\n",
    " - The word is \"trumps\" that was 184 times in real dataset, 4 times in fake dataset.\n",
    " - The word is \"says\" that was 156 times in real dataset, 41 times in fake dataset.\n",
    " - The word is \"donald\" that was 709 times in real dataset, 195 times in fake dataset.\n",
    " \n",
    "For Fake dataset:\n",
    " - The word is \"revolution\" that was 10 times in fake dataset, 1 time in real dataset.\n",
    " - The word is \"voter\" that was 26 times in fake dataset, 3 times in real dataset.\n",
    " - The word is \"american\" that was 19 times in fake dataset, 8 times in real dataset.\n",
    "    \n",
    "In the examples, there is at least 2 times the difference between the number of occurrences in the class to which the word belongs. With this reason, it may be possible to decide which category of the news title belongs.\n",
    "\n",
    "#### 2. Implementing Naive Bayes\n",
    "\n",
    "The Naive Bayes algorithm is a classifier algorithm that is based on the fact that the probabilities are independent. For the implementation of this assignment, the formulas on the slides of the course is used. The uni-gram and bi-gram formulas are given above:\n",
    "\n",
    "\\begin{equation*}\n",
    "P(w|c) = \\frac{count(w,c) + 1}{count(c) + |V|}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "P(w_i|w_{i-1}) = \\frac{count(w_iw_{i-1}) + 1}{count(w_{i-1}) + |V|}\n",
    "\\end{equation*}\n",
    "\n",
    "In formulas the laplase smoothing is used for the probability of words that are not in the train datasets should not be zero.\n",
    "#### 3.a. Analyzing effect of the words on prediction\n",
    "TF-IDF is simply the term consisting of the initials of Term Frequency - Inverse Document Frequency. It is based on the extraction of the terms in a text and making various calculations according to the amount of these terms.\n",
    "TF is term frequency which means the number of occurences of a word in a document over number of words in the document.\n",
    "IDF is term frequency which means the number of documents over the number of documents containing that word.\n",
    " - List the 10 words whose presence most strongly predicts that the news is real.\n",
    "     - trump : 0.08009378112018428\n",
    "     - donald : 0.07296814813860339\n",
    "     - to : 0.06658936964770612\n",
    "     - us : 0.06005947680616275\n",
    "     - trumps : 0.059955184421804086\n",
    "     - on : 0.059635366682539256\n",
    "     - in : 0.05941610834454733\n",
    "     - of : 0.05842431013367845\n",
    "     - says : 0.05836266806071561\n",
    "     - the : 0.05811208380071793\n",
    "\n",
    " - List the 10 words whose absence most strongly predicts that the news is real.\n",
    "     - deputy : 0.009646951595340398\n",
    "     - driving : 0.009646951595340398\n",
    "     - number : 0.009646951595340398\n",
    "     - complimentary : 0.009646951595340398\n",
    "     - walking : 0.009646951595340398\n",
    "     - wide : 0.009646951595340398\n",
    "     - decry : 0.009646951595340398\n",
    "     - guys : 0.009646951595340398\n",
    "     - barred : 0.009646951595340398\n",
    "     - boosting : 0.009646951595340398\n",
    " \n",
    " - List the 10 words whose presence most strongly predicts that the news is fake.\n",
    "     - trump : 0.077513728737285\n",
    "     - the : 0.0665253850620805\n",
    "     - to : 0.06603305486194166\n",
    "     - donald : 0.06055357739037391\n",
    "     - in : 0.0604540605312293\n",
    "     - of : 0.06025189503027582\n",
    "     - for : 0.05967306471624522\n",
    "     - and : 0.05821211781728385\n",
    "     - on : 0.05762690532130783\n",
    "     - is : 0.05663960249308482\n",
    " \n",
    " - List the 10 words whose absence most strongly predicts that the news is fake.\n",
    "     - shredded : 0.009653049840838524\n",
    "     - wide : 0.009653049840838524\n",
    "     - lahren : 0.009653049840838524\n",
    "     - legend : 0.009653049840838524\n",
    "     - concert : 0.009653049840838524\n",
    "     - directed : 0.009653049840838524\n",
    "     - plot : 0.009653049840838524\n",
    "     - microphone : 0.009653049840838524\n",
    "     - historischer : 0.009653049840838524\n",
    "     - molodets : 0.009653049840838524\n",
    "\n",
    "#### 3.b. Stopwords\n",
    " - List the 10 non-stopwords that most strongly predict that the news is real\n",
    "     - trump : 0.08694352463396378\n",
    "     - donald : 0.07920849654561526\n",
    "     - trumps : 0.06508264413049815\n",
    "     - says : 0.06335393331757595\n",
    "     - clinton : 0.05496210441768531\n",
    "     - election : 0.054811425936747565\n",
    "     - north : 0.05465854769238339\n",
    "     - ban : 0.054186047678400656\n",
    "     - korea : 0.053691216708490426\n",
    "     - president : 0.05352093696071042\n",
    " - List the 10 non-stopwords that most strongly predict that the news is fake.\n",
    "     - trump : 0.0860465787722201\n",
    "     - donald : 0.06721942360068997\n",
    "     - hillary : 0.06253969924497124\n",
    "     - clinton : 0.06137282560236677\n",
    "     - just : 0.055771793801039006\n",
    "     - election : 0.054766462525340835\n",
    "     - new : 0.053850034722354084\n",
    "     - obama : 0.05284783989475367\n",
    "     - president : 0.052635641379481776\n",
    "     - america : 0.05150663235171578 \n",
    "\n",
    "#### 3.c. Analyzing effect of the stopwords\n",
    "When we look at the TF-IDF values which takes account stopwords, it misleads on predictions. Because, generally stopwords are the mostly used words in documents (real and fake). In the fact of that the useage stopwords is not useful when interpreting the model. \n",
    "\n",
    "#### 4. Calculation of Accuracy\n",
    "- Accuracy of uni-gram which includes stopwords : %62.78118609406953\n",
    "- Accuracy of uni-gram which does not include stopwords : %60.940695296523515\n",
    "- Accuracy of bi-gram which includes stopwords : %81.39059304703477\n",
    "- Accuracy of bi-gram which does not include stopwords : %77.0961145194274"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import math\n",
    "import collections\n",
    "import pandas\n",
    "\n",
    "\"\"\"open and close the files\"\"\"\n",
    "fileOpenReal = open(\"clean_real-Train.txt\", \"r\")\n",
    "realTrain = fileOpenReal.read()\n",
    "fileOpenReal.close()\n",
    "fileOpenFake = open(\"clean_fake-Train.txt\", \"r\")\n",
    "fakeTrain = fileOpenFake.read()\n",
    "fileOpenFake.close()\n",
    "\n",
    "# create a variable as totalTrain that keep total train dataset\n",
    "totalTrain = realTrain + fakeTrain\n",
    "# token_pattern using for one character\n",
    "vecTotal = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
    "total = vecTotal.fit_transform([totalTrain])\n",
    "cardinality = len(total.toarray()[0])\n",
    "\n",
    "\"\"\"unigram for real train data\"\"\"\n",
    "vecReal = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
    "real = vecReal.fit_transform([realTrain])\n",
    "realUniq = vecReal.get_feature_names()\n",
    "realUniqCount = real.toarray()\n",
    "dicReal = dict(zip(realUniq, realUniqCount[0]))\n",
    "realCount = sum(realUniqCount[0])\n",
    "\n",
    "\"\"\"unigram for fake train data\"\"\"\n",
    "vecFake = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
    "fake = vecFake.fit_transform([fakeTrain])\n",
    "fakeUniq = vecFake.get_feature_names()\n",
    "fakeUniqCount = fake.toarray()\n",
    "dicFake = dict(zip(fakeUniq, fakeUniqCount[0]))\n",
    "fakeCount = sum(fakeUniqCount[0])\n",
    "\n",
    "\"\"\"find unigram probability\"\"\"\n",
    "dicReal_prob = {}\n",
    "dicFake_prob = {}\n",
    "for key, value in dicReal.items():\n",
    "    dicReal_prob[key] = math.log(((value + 1) / (realCount + cardinality)))\n",
    "\n",
    "for key, value in dicFake.items():\n",
    "    dicFake_prob[key] = math.log(((value + 1) / (fakeCount + cardinality)))\n",
    "\n",
    "\n",
    "vecBiTotal = CountVectorizer(ngram_range=(2, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "totalB = vecBiTotal.fit_transform([totalTrain])\n",
    "cardinalityB = len(totalB.toarray()[0])\n",
    "\n",
    "\"\"\"bigram for real train data\"\"\"\n",
    "vecBReal = CountVectorizer(ngram_range=(2, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "b_Real = vecBReal.fit_transform([realTrain])\n",
    "b_RealUniq = vecBReal.get_feature_names()\n",
    "b_RealUniqCount = b_Real.toarray()\n",
    "dicB_Real = dict(zip(b_RealUniq, b_RealUniqCount[0]))\n",
    "b_RealCount = sum(b_RealUniqCount[0])\n",
    "\n",
    "\"\"\"bigram for fake train data\"\"\"\n",
    "vecBFake = CountVectorizer(ngram_range=(2, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "b_Fake = vecBFake.fit_transform([fakeTrain])\n",
    "b_FakeUniq = vecBFake.get_feature_names()\n",
    "b_FakeUniqCount = b_Fake.toarray()\n",
    "dicB_Fake = dict(zip(b_FakeUniq, b_FakeUniqCount[0]))\n",
    "b_FakeCount = sum(b_FakeUniqCount[0])\n",
    "\n",
    "\"\"\"find bigram probability\"\"\"\n",
    "b_Real_Prob = {}\n",
    "for key, value in dicB_Real.items():\n",
    "    firstword = key.split()[0]\n",
    "    b_Real_Prob[key] = math.log((value + 1)/(dicReal[firstword] + cardinalityB))\n",
    "\n",
    "b_Fake_Prob = {}\n",
    "for key, value in dicB_Fake.items():\n",
    "    firstword = key.split()[0]\n",
    "    b_Fake_Prob[key] = math.log((value + 1)/(dicFake[firstword] + cardinalityB))\n",
    "\n",
    "realLineCount = len(realTrain.split('\\n'))\n",
    "fakeLineCount = len(fakeTrain.split('\\n'))\n",
    "realClass = math.log((realLineCount / (realLineCount + fakeLineCount)))\n",
    "fakeClass = math.log((fakeLineCount / (realLineCount + fakeLineCount)))\n",
    "\n",
    "\"\"\"Read test data\"\"\"\n",
    "testData = pandas.read_csv('test.csv', sep=',')\n",
    "count = 0\n",
    "\"\"\"accuracy for bigram\"\"\"\n",
    "for row in testData.itertuples():\n",
    "    prob_real = realClass\n",
    "    prob_fake = fakeClass\n",
    "    #print(row._2)\n",
    "    vecTestSentence = CountVectorizer(ngram_range=(2, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "    b_TestSentence = vecTestSentence.fit_transform([row.Id])\n",
    "    b_TestSentenceU = vecTestSentence.get_feature_names()\n",
    "\n",
    "    for j in range(len(b_TestSentenceU)):\n",
    "        if b_TestSentenceU[j] in dicB_Real:\n",
    "            prob_real += dicB_Real[b_TestSentenceU[j]]\n",
    "        else:\n",
    "            prob_real += math.log(1/cardinalityB)\n",
    "        if b_TestSentenceU[j] in dicB_Fake:\n",
    "            prob_fake += dicB_Fake[b_TestSentenceU[j]]\n",
    "        else:\n",
    "            prob_fake += math.log(1/cardinalityB)\n",
    "    if prob_real > prob_fake:\n",
    "        if row._2 == \"real\":\n",
    "            count += 1\n",
    "    elif prob_fake > prob_real:\n",
    "        if row._2 == \"fake\":\n",
    "            count += 1\n",
    "\n",
    "accuracy = 100*(count / len(testData))\n",
    "print(accuracy)\n",
    "\n",
    "\"\"\" TF-IDF\"\"\"\n",
    "vectReal = TfidfVectorizer(sublinear_tf=True, analyzer='word', ngram_range=(1, 1))\n",
    "tfidfReal = vectReal.fit_transform([realTrain])\n",
    "tfidfRealList = dict(zip(vectReal.get_feature_names(), tfidfReal.toarray()[0]))\n",
    "\n",
    "vectFake = TfidfVectorizer(sublinear_tf=True, analyzer='word', ngram_range=(1, 1))\n",
    "tfidfFake = vectFake.fit_transform([fakeTrain])\n",
    "tfidfFakeList = dict(zip(vectFake.get_feature_names(), tfidfFake.toarray()[0]))\n",
    "\n",
    "\n",
    "\"\"\"TF-IDF stopwords\"\"\"\n",
    "vectRealStop = TfidfVectorizer(sublinear_tf=True, analyzer='word', stop_words=ENGLISH_STOP_WORDS, ngram_range=(1, 1))\n",
    "tfidfRealStop = vectRealStop.fit_transform([realTrain])\n",
    "tfidfRealListStop = dict(zip(vectRealStop.get_feature_names(), tfidfRealStop.toarray()[0]))\n",
    "\n",
    "vectFakeStop = TfidfVectorizer(sublinear_tf=True, analyzer='word', stop_words= ENGLISH_STOP_WORDS, ngram_range=(1, 1))\n",
    "tfidfFakeStop = vectFakeStop.fit_transform([fakeTrain])\n",
    "tfidfFakeListStop = dict(zip(vectFakeStop.get_feature_names(), tfidfFakeStop.toarray()[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
